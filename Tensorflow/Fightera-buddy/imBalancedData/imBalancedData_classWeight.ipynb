{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.10.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = make_classification(n_samples = 100000,\n",
    "                           n_features= 10,\n",
    "                           n_classes= 2,\n",
    "                           weights= [0.99, 0.01],\n",
    "                           n_redundant = 0)\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.bar(unique, counts)\n",
    "\n",
    "plt.xticks(unique, fontsize = 16)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.title(\"Class Distribution\", fontsize = 16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size= 0.2 , stratify= y , random_state= 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5072987609227765, 1: 34.75238922675934}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight= 'balanced',\n",
    "                                     classes = [0, 1], # classes = np.unique(y_train)\n",
    "                                     y = y_train)\n",
    "\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                704       \n",
      "                                                                 \n",
      " tf.nn.relu (TFOpLambda)     (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " tf.nn.relu_1 (TFOpLambda)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,817\n",
      "Trainable params: 2,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.Input(shape=(X.shape[1],))\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(input_layer)\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(32)(x)\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(1)(x)\n",
    "x = tf.keras.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "\n",
    "base_model = tf.keras.models.Model(input_layer, x)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model(np.expand_dims(X_train[0] ,axis = 0)) # You can call this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-9.70271230e-03, -1.40596285e-01,  2.34953433e-01,\n",
       "         -2.52574980e-01, -3.36275697e-02,  2.32955009e-01,\n",
       "          4.72059250e-02, -1.08968750e-01, -5.08068949e-02,\n",
       "          5.49819171e-02, -1.07339606e-01,  1.74484879e-01,\n",
       "          1.72162950e-01,  2.63536066e-01, -3.83940488e-02,\n",
       "          2.80253619e-01, -1.64512187e-01, -1.60035491e-03,\n",
       "         -1.10410705e-01,  2.64425248e-01,  2.43144065e-01,\n",
       "         -4.88453656e-02, -2.75680602e-01,  2.61161029e-02,\n",
       "         -2.07351208e-01,  1.32316679e-01,  2.35888451e-01,\n",
       "          1.79418862e-01, -1.60548419e-01,  4.11699116e-02,\n",
       "          1.20817572e-01, -2.76718497e-01,  8.10914636e-02,\n",
       "         -1.24829739e-01,  9.25541818e-02, -1.96107090e-01,\n",
       "         -8.35102797e-02, -1.80555254e-01, -2.31449306e-01,\n",
       "          5.30472398e-02, -8.83296579e-02, -8.02665949e-04,\n",
       "          1.63622350e-01,  1.12616420e-01,  2.61609167e-01,\n",
       "          7.96136558e-02, -1.31003574e-01, -1.02024242e-01,\n",
       "         -2.12814853e-01, -6.27625585e-02, -1.26946509e-01,\n",
       "          1.09172463e-02, -1.04019046e-02, -1.55960470e-01,\n",
       "         -5.57388961e-02, -2.58315057e-01,  2.34030813e-01,\n",
       "         -1.99187130e-01, -1.71165645e-01, -1.50596008e-01,\n",
       "         -1.24232516e-01,  1.83928460e-01, -2.59567201e-01,\n",
       "         -4.45999205e-02],\n",
       "        [-2.38227367e-01, -1.13164634e-01,  3.49865556e-02,\n",
       "          1.23777181e-01,  2.82345504e-01,  2.20388144e-01,\n",
       "          1.87160254e-01, -2.14417905e-01, -1.10540390e-02,\n",
       "          2.61702329e-01,  2.59045273e-01, -6.94137812e-03,\n",
       "         -2.03070045e-03,  1.36834353e-01, -1.83326751e-01,\n",
       "         -1.40977144e-01, -2.13701621e-01,  1.57115459e-01,\n",
       "         -2.60714173e-01, -1.23692125e-01, -1.54805675e-01,\n",
       "         -2.13963062e-01, -3.38843167e-02,  9.28522050e-02,\n",
       "          5.90998232e-02,  1.76743001e-01,  1.40041709e-02,\n",
       "         -1.63795829e-01, -2.45308280e-02, -2.84658730e-01,\n",
       "          1.76182240e-01,  1.68198228e-01, -3.79366130e-02,\n",
       "         -1.80081397e-01, -2.27808893e-01,  1.10561341e-01,\n",
       "         -2.69010425e-01, -1.64084420e-01, -1.80153966e-01,\n",
       "         -4.41824794e-02, -2.74766684e-01, -8.87370557e-02,\n",
       "         -3.69757861e-02,  1.91020548e-01,  2.48101354e-03,\n",
       "         -3.15302610e-02, -2.20835805e-01, -5.79610467e-03,\n",
       "         -5.95296770e-02,  1.87560737e-01, -1.29908249e-01,\n",
       "         -1.53990060e-01, -1.04784131e-01,  1.49271220e-01,\n",
       "         -2.80440181e-01,  2.37112492e-01, -2.60013491e-01,\n",
       "          2.16980606e-01,  1.94418013e-01, -1.98813200e-01,\n",
       "          1.58822268e-01, -2.07655728e-02, -6.78618401e-02,\n",
       "         -8.15063268e-02],\n",
       "        [-5.48101068e-02, -1.19149521e-01,  4.01491821e-02,\n",
       "          1.41734779e-01, -2.42856920e-01, -2.24890411e-01,\n",
       "          1.94945008e-01,  1.21723711e-02, -5.61574996e-02,\n",
       "          2.35496253e-01, -2.25729391e-01,  2.55781323e-01,\n",
       "          1.36237741e-02, -2.43259847e-01,  2.75659829e-01,\n",
       "          1.12049818e-01,  2.57806927e-01, -2.62068778e-01,\n",
       "         -1.57036856e-01, -4.44947034e-02,  1.21251166e-01,\n",
       "         -4.53880578e-02,  8.54013860e-02, -1.75470114e-02,\n",
       "         -2.81037450e-01, -1.70544475e-01, -1.45077512e-01,\n",
       "         -8.22928846e-02, -1.34687364e-01,  2.42747694e-01,\n",
       "         -1.15559205e-01,  5.05989790e-03,  1.96112722e-01,\n",
       "         -1.93705708e-01, -1.83129311e-02,  1.84055626e-01,\n",
       "         -2.40153164e-01,  1.37621939e-01, -1.41097039e-01,\n",
       "          1.80437386e-01,  1.76600188e-01, -1.76842272e-01,\n",
       "         -2.68317550e-01,  3.53851914e-03,  2.32945591e-01,\n",
       "          2.47557372e-01,  1.44208461e-01,  4.00387347e-02,\n",
       "         -2.56326169e-01, -3.68582606e-02,  1.55658782e-01,\n",
       "          5.65555394e-02,  4.55896258e-02,  6.12142682e-03,\n",
       "          2.10013956e-01,  1.80765510e-01, -1.52137637e-01,\n",
       "          2.50629455e-01, -1.97209463e-01, -2.07859829e-01,\n",
       "         -6.96542859e-03, -1.46434620e-01, -5.08973151e-02,\n",
       "          1.18297309e-01],\n",
       "        [-6.06719851e-02,  2.72985667e-01, -3.86391282e-02,\n",
       "         -1.95003271e-01,  1.14520580e-01,  4.47583795e-02,\n",
       "         -2.49337345e-01,  7.68758953e-02, -2.74772853e-01,\n",
       "         -9.25928205e-02,  1.03683501e-01, -1.71833411e-01,\n",
       "         -1.15180939e-01,  2.27142841e-01,  1.76781714e-01,\n",
       "          1.72585547e-02, -1.66593596e-01, -2.16464490e-01,\n",
       "         -2.77763158e-01,  1.63962901e-01,  9.15754139e-02,\n",
       "          5.75691760e-02,  6.55259788e-02,  2.17040271e-01,\n",
       "          2.21502215e-01,  1.64569676e-01,  6.27883673e-02,\n",
       "         -2.09608853e-01,  2.08822787e-02, -2.79730052e-01,\n",
       "         -2.59538352e-01, -1.71350926e-01,  1.03552490e-01,\n",
       "         -7.16686547e-02,  2.11670041e-01,  1.46124423e-01,\n",
       "          3.56629491e-03,  7.40482509e-02,  2.38502294e-01,\n",
       "         -2.13061154e-01, -4.53896075e-02, -2.60316074e-01,\n",
       "         -2.81199574e-01,  1.13277733e-02, -2.08264858e-01,\n",
       "         -1.70623764e-01, -2.43371114e-01, -2.70065576e-01,\n",
       "         -1.57936454e-01,  3.00531387e-02, -2.65358269e-01,\n",
       "          2.70797461e-01,  2.72378474e-01, -1.43294618e-01,\n",
       "         -1.50881827e-01,  1.16249442e-01,  2.77374595e-01,\n",
       "         -1.34098157e-01, -1.92326337e-01, -2.68282056e-01,\n",
       "          1.82894707e-01, -1.56795785e-01, -5.31570762e-02,\n",
       "         -4.01096791e-02],\n",
       "        [ 2.31611341e-01,  6.67321682e-02,  2.21476406e-01,\n",
       "         -1.17035180e-01,  7.21121132e-02, -1.40316382e-01,\n",
       "          1.89541727e-01,  3.35362554e-02,  2.70842105e-01,\n",
       "         -2.42827863e-01,  1.82048738e-01,  2.12209225e-01,\n",
       "         -2.36800194e-01, -2.10409001e-01,  2.14490414e-01,\n",
       "         -9.46335644e-02,  2.56969720e-01, -2.06351340e-01,\n",
       "          2.15633541e-01,  8.35423768e-02, -2.03478128e-01,\n",
       "          2.44129688e-01,  2.07282096e-01,  1.66798890e-01,\n",
       "         -1.37691587e-01,  2.47389346e-01, -1.09658092e-01,\n",
       "          1.07787609e-01, -2.06115633e-01, -1.83506399e-01,\n",
       "          2.87264585e-02, -2.82635510e-01, -1.39227167e-01,\n",
       "          2.20547050e-01, -1.06673554e-01, -3.34568918e-02,\n",
       "          1.11829460e-01, -1.99506283e-01,  2.66134173e-01,\n",
       "         -2.34410092e-01,  2.22151250e-01,  4.12481725e-02,\n",
       "         -1.20240360e-01, -6.18762672e-02,  2.09191889e-01,\n",
       "         -2.42522299e-01, -2.18571708e-01,  2.60027081e-01,\n",
       "          6.61794841e-02, -1.52576342e-01,  2.29491264e-01,\n",
       "          1.94680512e-01,  2.23565429e-01, -6.39764220e-02,\n",
       "         -2.79246598e-01, -3.86619419e-02, -1.45927683e-01,\n",
       "          1.48490906e-01,  1.21759057e-01,  1.68101221e-01,\n",
       "         -2.40509108e-01,  2.50602096e-01, -2.58519590e-01,\n",
       "          7.42717385e-02],\n",
       "        [ 4.88894880e-02, -2.13020205e-01,  1.52110606e-01,\n",
       "          1.15665942e-01, -2.71827966e-01, -1.77198648e-02,\n",
       "         -2.78196841e-01,  1.75715923e-01, -2.03266650e-01,\n",
       "          1.99141711e-01, -1.76348567e-01, -1.61535323e-01,\n",
       "          2.23378837e-02,  5.23009896e-03, -7.40459561e-03,\n",
       "         -2.44460523e-01, -1.63081020e-01, -1.42844617e-02,\n",
       "         -2.37588257e-01, -2.37590075e-03,  2.77884990e-01,\n",
       "         -5.31286001e-04,  2.50923336e-02,  1.37890220e-01,\n",
       "         -2.86298394e-02,  2.75971740e-01, -2.29073882e-01,\n",
       "         -1.12895384e-01,  9.21478868e-03, -1.99403644e-01,\n",
       "         -4.40513790e-02,  1.05123430e-01, -1.16987228e-02,\n",
       "         -2.46928424e-01, -2.55281091e-01, -1.27541557e-01,\n",
       "         -2.18708709e-01, -4.37500179e-02, -2.52426028e-01,\n",
       "         -1.05622411e-03, -1.86959565e-01,  1.86097980e-01,\n",
       "          1.99593872e-01,  6.09970391e-02, -1.25756696e-01,\n",
       "          2.34964818e-01,  8.33221674e-02,  1.81064755e-01,\n",
       "         -2.76058257e-01,  2.54756898e-01,  1.19426370e-01,\n",
       "         -1.68234110e-04, -1.22762308e-01, -1.84908211e-02,\n",
       "         -1.35811269e-01,  1.77807510e-01,  1.70112699e-01,\n",
       "         -2.45436639e-01,  9.91855264e-02,  2.67127603e-01,\n",
       "          1.95184946e-01, -2.40173742e-01,  1.74438775e-01,\n",
       "          1.65916681e-02],\n",
       "        [-6.14446998e-02,  1.88526452e-01,  2.56675214e-01,\n",
       "         -1.29731536e-01, -1.30012661e-01,  2.25208312e-01,\n",
       "          2.05994099e-01, -1.50984406e-01,  1.74071908e-01,\n",
       "         -6.93027228e-02, -1.06521204e-01, -2.78995156e-01,\n",
       "         -2.77897716e-02,  8.93128216e-02,  2.22372144e-01,\n",
       "         -2.83159345e-01,  1.83294415e-02, -1.34122372e-03,\n",
       "          8.78053308e-02,  2.70571500e-01,  8.20404291e-03,\n",
       "          1.30435348e-01,  5.18833399e-02,  2.78962046e-01,\n",
       "          1.17655545e-01, -1.61605775e-02,  2.52500564e-01,\n",
       "         -2.06564575e-01,  1.72974706e-01, -2.13591635e-01,\n",
       "         -2.09869757e-01, -1.10771671e-01, -9.43969637e-02,\n",
       "          9.67282057e-03,  8.92275572e-02, -2.05796689e-01,\n",
       "         -2.05505162e-01,  2.55226225e-01,  2.34042257e-01,\n",
       "         -1.61461651e-01,  1.63412452e-01, -5.72654456e-02,\n",
       "         -2.01042622e-01, -3.52615863e-02, -1.76265836e-02,\n",
       "         -1.20657548e-01,  5.42756915e-02, -1.45697877e-01,\n",
       "         -1.19172201e-01,  1.54201478e-01, -2.67876685e-02,\n",
       "          1.61940873e-01, -1.44592926e-01,  2.11375535e-01,\n",
       "         -1.64583474e-01, -9.24396515e-03, -6.23339713e-02,\n",
       "         -3.85034829e-02, -6.68593198e-02,  1.44116014e-01,\n",
       "         -6.71627969e-02, -1.54162362e-01,  8.07715058e-02,\n",
       "          1.78909957e-01],\n",
       "        [ 2.52731949e-01,  1.71142220e-01,  1.67542815e-01,\n",
       "          2.20555276e-01,  1.25151992e-01,  1.60364211e-02,\n",
       "         -2.02746972e-01, -1.63699687e-01, -3.94514948e-02,\n",
       "          2.78177768e-01,  1.52540833e-01,  2.66174585e-01,\n",
       "          9.11052227e-02,  9.25259292e-02, -2.44674385e-01,\n",
       "          6.07851446e-02,  7.37337172e-02,  1.98119164e-01,\n",
       "         -1.50416374e-01,  1.26169324e-01, -2.29370594e-03,\n",
       "          2.28142053e-01, -2.47414917e-01, -3.04369628e-02,\n",
       "          1.21162176e-01, -2.49568358e-01,  1.73585981e-01,\n",
       "          2.07585096e-01,  1.02567554e-01, -9.54961628e-02,\n",
       "          1.05186433e-01, -1.55777574e-01,  2.43714154e-02,\n",
       "          2.59411484e-01,  2.02578008e-01, -1.03343114e-01,\n",
       "         -2.25542620e-01,  2.21831650e-01, -2.18026429e-01,\n",
       "          1.65045857e-01,  1.44815803e-01, -2.16113657e-01,\n",
       "          1.11533105e-01,  5.97628951e-02, -1.61908567e-01,\n",
       "          1.60876304e-01,  2.26564109e-02,  8.97134244e-02,\n",
       "         -1.87454283e-01,  6.71135783e-02, -1.17823988e-01,\n",
       "          1.39230222e-01, -2.25265443e-01, -2.40531176e-01,\n",
       "          2.64985114e-01,  7.12097287e-02,  1.11804754e-01,\n",
       "          7.02457130e-02, -2.63685763e-01, -1.22938216e-01,\n",
       "          2.50653177e-01, -1.87818915e-01,  1.00368887e-01,\n",
       "          2.22623050e-02],\n",
       "        [ 2.51522809e-01,  1.37466550e-01,  8.09903145e-02,\n",
       "          8.72804224e-02,  2.80294746e-01, -1.53633237e-01,\n",
       "          2.54906029e-01,  6.01238608e-02,  1.08676612e-01,\n",
       "         -1.14692464e-01, -7.22751766e-02,  2.56867379e-01,\n",
       "          1.85223371e-01,  1.18662357e-01, -2.20782578e-01,\n",
       "          1.43963128e-01, -2.19545648e-01, -8.94272774e-02,\n",
       "         -1.04699269e-01, -2.29394510e-01,  8.29133391e-02,\n",
       "          4.22884524e-02, -1.33687690e-01,  2.78816789e-01,\n",
       "          2.50529140e-01,  2.26046890e-01, -2.76927650e-02,\n",
       "          1.43355787e-01,  2.55489320e-01,  1.08443826e-01,\n",
       "         -1.74944833e-01, -2.55436838e-01, -5.41697741e-02,\n",
       "         -2.39845768e-01,  7.67418742e-02,  2.35587269e-01,\n",
       "          1.47863001e-01, -9.50983912e-02,  1.53490722e-01,\n",
       "         -7.48354793e-02, -1.98026374e-01,  1.94495589e-01,\n",
       "          1.95456147e-02, -8.41119736e-02, -1.53732494e-01,\n",
       "         -2.05226958e-01,  1.41454875e-01,  4.89708185e-02,\n",
       "          2.11378723e-01,  5.14827967e-02, -1.45819485e-02,\n",
       "         -2.69527137e-02, -9.15074050e-02, -1.91705629e-01,\n",
       "          2.28106588e-01,  1.30227864e-01, -2.72959888e-01,\n",
       "         -1.58620223e-01, -1.24048606e-01, -2.59832710e-01,\n",
       "         -1.17502064e-01,  3.95753086e-02,  1.30201399e-01,\n",
       "         -2.29415625e-01],\n",
       "        [-1.45965636e-01, -2.36466318e-01, -1.76952034e-01,\n",
       "         -5.72147965e-02,  1.96731687e-02, -4.02660966e-02,\n",
       "          2.61637896e-01, -2.51525819e-01,  3.81881297e-02,\n",
       "         -1.40370563e-01,  1.03316426e-01, -1.47813648e-01,\n",
       "         -1.96126565e-01,  1.46312684e-01, -3.19817364e-02,\n",
       "         -9.35089141e-02, -1.96906686e-01,  2.67225176e-01,\n",
       "         -2.20441043e-01,  1.07430935e-01,  1.21598154e-01,\n",
       "          9.09670889e-02, -9.13995951e-02,  2.01757759e-01,\n",
       "          2.19861180e-01,  2.09412456e-01,  7.77546465e-02,\n",
       "         -8.15531611e-03,  9.16075706e-03, -2.10712403e-01,\n",
       "          1.29940361e-01,  1.08677983e-01, -1.70866400e-01,\n",
       "          3.50886583e-02,  2.38735467e-01,  2.80304044e-01,\n",
       "         -2.14038208e-01,  2.46781200e-01, -1.08871460e-01,\n",
       "          7.18357861e-02, -1.10505953e-01,  1.82060003e-01,\n",
       "         -2.42662489e-01,  6.53453469e-02,  1.51619315e-01,\n",
       "         -2.72525251e-01,  1.72356158e-01, -9.94859338e-02,\n",
       "         -2.81993270e-01, -1.56693399e-01, -9.02627259e-02,\n",
       "         -2.04695851e-01,  5.01225591e-02,  2.37227827e-01,\n",
       "          5.15055954e-02, -3.30888033e-02, -5.41005880e-02,\n",
       "         -1.25427559e-01, -2.33316422e-02, -2.77710348e-01,\n",
       "          1.11274868e-01, -2.43660450e-01,  2.40693420e-01,\n",
       "          2.59988815e-01]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[-0.03293234, -0.05348313,  0.04383254, ..., -0.11644566,\n",
       "          0.11100292, -0.1695447 ],\n",
       "        [ 0.04411274, -0.09096771, -0.08731204, ..., -0.00425762,\n",
       "          0.07289797, -0.08314383],\n",
       "        [ 0.06502682,  0.09301674, -0.0343436 , ...,  0.06796682,\n",
       "          0.01294547,  0.12869   ],\n",
       "        ...,\n",
       "        [ 0.05675846, -0.0216403 , -0.09471756, ...,  0.22895539,\n",
       "         -0.06812692, -0.22252029],\n",
       "        [-0.24039191, -0.22818506, -0.17428046, ..., -0.05071449,\n",
       "          0.08170682,  0.17819959],\n",
       "        [ 0.18871838, -0.18256515,  0.14020556, ..., -0.06733781,\n",
       "         -0.04966927,  0.01727074]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       dtype=float32),\n",
       " array([[ 0.22652018],\n",
       "        [ 0.08180684],\n",
       "        [ 0.13185006],\n",
       "        [ 0.08587289],\n",
       "        [-0.320659  ],\n",
       "        [-0.09797221],\n",
       "        [-0.13962466],\n",
       "        [ 0.06718761],\n",
       "        [ 0.24417722],\n",
       "        [ 0.11021811],\n",
       "        [-0.2601026 ],\n",
       "        [-0.21057479],\n",
       "        [-0.27089423],\n",
       "        [ 0.0244427 ],\n",
       "        [ 0.15829754],\n",
       "        [-0.21698448],\n",
       "        [-0.25274348],\n",
       "        [-0.14385644],\n",
       "        [-0.3860587 ],\n",
       "        [ 0.19425547],\n",
       "        [-0.22311856],\n",
       "        [ 0.05788982],\n",
       "        [ 0.02706161],\n",
       "        [-0.03920099],\n",
       "        [ 0.28098673],\n",
       "        [-0.24722649],\n",
       "        [-0.14076489],\n",
       "        [-0.32383767],\n",
       "        [-0.08007634],\n",
       "        [ 0.25538814],\n",
       "        [ 0.09341747],\n",
       "        [-0.14784291]], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save_weights(\"initial_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_model = tf.keras.models.clone_model(base_model)\n",
    "focal_loss_model = tf.keras.models.clone_model(base_model)\n",
    "cat_focal_loss_model = tf.keras.models.clone_model(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = [tf.keras.metrics.Precision(),\n",
    "                              tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "500/500 [==============================] - 2s 2ms/step - loss: 0.0896 - precision: 0.0097 - recall: 0.0022 - val_loss: 0.0645 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0644 - precision: 0.7273 - recall: 0.0344 - val_loss: 0.0622 - val_precision: 0.5667 - val_recall: 0.0766\n",
      "Epoch 3/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0616 - precision: 0.6412 - recall: 0.1173 - val_loss: 0.0603 - val_precision: 0.5510 - val_recall: 0.1216\n",
      "Epoch 4/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0602 - precision: 0.6413 - recall: 0.1539 - val_loss: 0.0585 - val_precision: 0.6154 - val_recall: 0.1441\n",
      "Epoch 5/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0581 - precision: 0.6774 - recall: 0.1808 - val_loss: 0.0560 - val_precision: 0.5968 - val_recall: 0.1667\n",
      "Epoch 6/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0557 - precision: 0.7143 - recall: 0.2099 - val_loss: 0.0543 - val_precision: 0.6825 - val_recall: 0.1937\n",
      "Epoch 7/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0545 - precision: 0.7053 - recall: 0.2293 - val_loss: 0.0533 - val_precision: 0.6765 - val_recall: 0.2072\n",
      "Epoch 8/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0541 - precision: 0.7255 - recall: 0.2390 - val_loss: 0.0525 - val_precision: 0.6712 - val_recall: 0.2207\n",
      "Epoch 9/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0534 - precision: 0.7179 - recall: 0.2465 - val_loss: 0.0526 - val_precision: 0.5938 - val_recall: 0.2568\n",
      "Epoch 10/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0534 - precision: 0.7262 - recall: 0.2540 - val_loss: 0.0524 - val_precision: 0.6761 - val_recall: 0.2162\n",
      "Epoch 11/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0526 - precision: 0.7275 - recall: 0.2702 - val_loss: 0.0521 - val_precision: 0.6296 - val_recall: 0.2297\n",
      "Epoch 12/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0516 - precision: 0.7294 - recall: 0.2670 - val_loss: 0.0519 - val_precision: 0.6933 - val_recall: 0.2342\n",
      "Epoch 13/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0516 - precision: 0.7493 - recall: 0.2766 - val_loss: 0.0515 - val_precision: 0.6901 - val_recall: 0.2207\n",
      "Epoch 14/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0510 - precision: 0.7595 - recall: 0.2788 - val_loss: 0.0521 - val_precision: 0.5714 - val_recall: 0.2883\n",
      "Epoch 15/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0509 - precision: 0.7232 - recall: 0.2756 - val_loss: 0.0510 - val_precision: 0.7467 - val_recall: 0.2523\n",
      "Epoch 16/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0506 - precision: 0.7790 - recall: 0.3036 - val_loss: 0.0521 - val_precision: 0.7123 - val_recall: 0.2342\n",
      "Epoch 17/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0506 - precision: 0.7815 - recall: 0.3003 - val_loss: 0.0508 - val_precision: 0.6400 - val_recall: 0.2883\n",
      "Epoch 18/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0502 - precision: 0.7730 - recall: 0.3079 - val_loss: 0.0511 - val_precision: 0.6559 - val_recall: 0.2748\n",
      "Epoch 19/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0496 - precision: 0.7798 - recall: 0.3165 - val_loss: 0.0508 - val_precision: 0.6813 - val_recall: 0.2793\n",
      "Epoch 20/32\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0500 - precision: 0.7663 - recall: 0.3036 - val_loss: 0.0504 - val_precision: 0.6818 - val_recall: 0.2703\n",
      "Epoch 21/32\n",
      "500/500 [==============================] - 1s 1ms/step - loss: 0.0493 - precision: 0.7696 - recall: 0.3057 - val_loss: 0.0504 - val_precision: 0.6598 - val_recall: 0.2883\n",
      "Epoch 22/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0492 - precision: 0.7781 - recall: 0.3208 - val_loss: 0.0507 - val_precision: 0.6860 - val_recall: 0.2658\n",
      "Epoch 23/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0491 - precision: 0.7849 - recall: 0.3143 - val_loss: 0.0501 - val_precision: 0.6957 - val_recall: 0.2883\n",
      "Epoch 24/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0493 - precision: 0.8016 - recall: 0.3175 - val_loss: 0.0503 - val_precision: 0.7237 - val_recall: 0.2477\n",
      "Epoch 25/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0486 - precision: 0.7932 - recall: 0.3262 - val_loss: 0.0501 - val_precision: 0.6813 - val_recall: 0.2793\n",
      "Epoch 26/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0482 - precision: 0.8082 - recall: 0.3402 - val_loss: 0.0501 - val_precision: 0.7126 - val_recall: 0.2793\n",
      "Epoch 27/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0484 - precision: 0.8015 - recall: 0.3391 - val_loss: 0.0507 - val_precision: 0.7089 - val_recall: 0.2523\n",
      "Epoch 28/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0484 - precision: 0.8005 - recall: 0.3369 - val_loss: 0.0499 - val_precision: 0.7011 - val_recall: 0.2748\n",
      "Epoch 29/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0479 - precision: 0.7995 - recall: 0.3305 - val_loss: 0.0504 - val_precision: 0.6602 - val_recall: 0.3063\n",
      "Epoch 30/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0478 - precision: 0.8005 - recall: 0.3326 - val_loss: 0.0506 - val_precision: 0.6957 - val_recall: 0.2883\n",
      "Epoch 31/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0474 - precision: 0.8204 - recall: 0.3541 - val_loss: 0.0500 - val_precision: 0.7439 - val_recall: 0.2748\n",
      "Epoch 32/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0475 - precision: 0.8105 - recall: 0.3315 - val_loss: 0.0502 - val_precision: 0.7093 - val_recall: 0.2748\n"
     ]
    }
   ],
   "source": [
    "history = base_model.fit(X_train, y_train, epochs = 32, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.0516 - precision: 0.7241 - recall: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05155213177204132, 0.7241379022598267, 0.2916666567325592]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 568us/step\n"
     ]
    }
   ],
   "source": [
    "preds = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19712\n",
      "           1       0.72      0.29      0.42       288\n",
      "\n",
      "    accuracy                           0.99     20000\n",
      "   macro avg       0.86      0.65      0.70     20000\n",
      "weighted avg       0.99      0.99      0.99     20000\n",
      "\n",
      "[[19680    32]\n",
      " [  204    84]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "y_pred = np.where(preds >= threshold , 1 , 0)\n",
    "\n",
    "print(classification_report(y_test,  y_pred))\n",
    "print(confusion_matrix(y_test,  y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     19712\n",
      "           1       0.37      0.47      0.41       288\n",
      "\n",
      "    accuracy                           0.98     20000\n",
      "   macro avg       0.68      0.73      0.70     20000\n",
      "weighted avg       0.98      0.98      0.98     20000\n",
      "\n",
      "[[19478   234]\n",
      " [  152   136]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "y_pred = np.where(preds >= threshold , 1 , 0)\n",
    "\n",
    "print(classification_report(y_test,  y_pred))\n",
    "print(confusion_matrix(y_test,  y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_model.load_weights(\"initial_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5072987609227765, 1: 34.75238922675934}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.5584 - precision_1: 0.0430 - recall_1: 0.6383 - val_loss: 0.4697 - val_precision_1: 0.0542 - val_recall_1: 0.6892\n",
      "Epoch 2/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4997 - precision_1: 0.0602 - recall_1: 0.6760 - val_loss: 0.4580 - val_precision_1: 0.0672 - val_recall_1: 0.6577\n",
      "Epoch 3/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4917 - precision_1: 0.0689 - recall_1: 0.6792 - val_loss: 0.4290 - val_precision_1: 0.0738 - val_recall_1: 0.6216\n",
      "Epoch 4/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4869 - precision_1: 0.0728 - recall_1: 0.6685 - val_loss: 0.4584 - val_precision_1: 0.0704 - val_recall_1: 0.6532\n",
      "Epoch 5/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4785 - precision_1: 0.0749 - recall_1: 0.6738 - val_loss: 0.4562 - val_precision_1: 0.0800 - val_recall_1: 0.6216\n",
      "Epoch 6/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4736 - precision_1: 0.0842 - recall_1: 0.6620 - val_loss: 0.4548 - val_precision_1: 0.0715 - val_recall_1: 0.6441\n",
      "Epoch 7/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4731 - precision_1: 0.0835 - recall_1: 0.6771 - val_loss: 0.4904 - val_precision_1: 0.0638 - val_recall_1: 0.6802\n",
      "Epoch 8/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4730 - precision_1: 0.0807 - recall_1: 0.6760 - val_loss: 0.4188 - val_precision_1: 0.0862 - val_recall_1: 0.6351\n",
      "Epoch 9/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4624 - precision_1: 0.0844 - recall_1: 0.6846 - val_loss: 0.3384 - val_precision_1: 0.1090 - val_recall_1: 0.5946\n",
      "Epoch 10/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4611 - precision_1: 0.0877 - recall_1: 0.6771 - val_loss: 0.4247 - val_precision_1: 0.0895 - val_recall_1: 0.6396\n",
      "Epoch 11/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4594 - precision_1: 0.0886 - recall_1: 0.6825 - val_loss: 0.4736 - val_precision_1: 0.0730 - val_recall_1: 0.6486\n",
      "Epoch 12/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4529 - precision_1: 0.0897 - recall_1: 0.6889 - val_loss: 0.3421 - val_precision_1: 0.1189 - val_recall_1: 0.6081\n",
      "Epoch 13/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4487 - precision_1: 0.0925 - recall_1: 0.6792 - val_loss: 0.3949 - val_precision_1: 0.1193 - val_recall_1: 0.6126\n",
      "Epoch 14/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4469 - precision_1: 0.0958 - recall_1: 0.6846 - val_loss: 0.4057 - val_precision_1: 0.0951 - val_recall_1: 0.6351\n",
      "Epoch 15/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4427 - precision_1: 0.0983 - recall_1: 0.6900 - val_loss: 0.4367 - val_precision_1: 0.0862 - val_recall_1: 0.6351\n",
      "Epoch 16/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4398 - precision_1: 0.0962 - recall_1: 0.6889 - val_loss: 0.4009 - val_precision_1: 0.0921 - val_recall_1: 0.6441\n",
      "Epoch 17/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4393 - precision_1: 0.0986 - recall_1: 0.6932 - val_loss: 0.3625 - val_precision_1: 0.1103 - val_recall_1: 0.6171\n",
      "Epoch 18/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4350 - precision_1: 0.1008 - recall_1: 0.6878 - val_loss: 0.4240 - val_precision_1: 0.0767 - val_recall_1: 0.6396\n",
      "Epoch 19/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4324 - precision_1: 0.0997 - recall_1: 0.6911 - val_loss: 0.4339 - val_precision_1: 0.0841 - val_recall_1: 0.6577\n",
      "Epoch 20/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4319 - precision_1: 0.1006 - recall_1: 0.6932 - val_loss: 0.3778 - val_precision_1: 0.1012 - val_recall_1: 0.6306\n",
      "Epoch 21/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4279 - precision_1: 0.0994 - recall_1: 0.6900 - val_loss: 0.4377 - val_precision_1: 0.0917 - val_recall_1: 0.6441\n",
      "Epoch 22/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4281 - precision_1: 0.1024 - recall_1: 0.6889 - val_loss: 0.4194 - val_precision_1: 0.0935 - val_recall_1: 0.6306\n",
      "Epoch 23/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4230 - precision_1: 0.1009 - recall_1: 0.6868 - val_loss: 0.3982 - val_precision_1: 0.0881 - val_recall_1: 0.6261\n",
      "Epoch 24/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4193 - precision_1: 0.1030 - recall_1: 0.6911 - val_loss: 0.4257 - val_precision_1: 0.0830 - val_recall_1: 0.6486\n",
      "Epoch 25/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4210 - precision_1: 0.1027 - recall_1: 0.6943 - val_loss: 0.4023 - val_precision_1: 0.0894 - val_recall_1: 0.6441\n",
      "Epoch 26/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4128 - precision_1: 0.1020 - recall_1: 0.7040 - val_loss: 0.4167 - val_precision_1: 0.0790 - val_recall_1: 0.6441\n",
      "Epoch 27/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4144 - precision_1: 0.1040 - recall_1: 0.7094 - val_loss: 0.4017 - val_precision_1: 0.0834 - val_recall_1: 0.6441\n",
      "Epoch 28/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4063 - precision_1: 0.1013 - recall_1: 0.7137 - val_loss: 0.3072 - val_precision_1: 0.1200 - val_recall_1: 0.5946\n",
      "Epoch 29/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4051 - precision_1: 0.1046 - recall_1: 0.7115 - val_loss: 0.3734 - val_precision_1: 0.0921 - val_recall_1: 0.6171\n",
      "Epoch 30/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4040 - precision_1: 0.1022 - recall_1: 0.7234 - val_loss: 0.3372 - val_precision_1: 0.1150 - val_recall_1: 0.6081\n",
      "Epoch 31/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.4036 - precision_1: 0.1024 - recall_1: 0.7104 - val_loss: 0.4049 - val_precision_1: 0.0870 - val_recall_1: 0.6306\n",
      "Epoch 32/32\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.3973 - precision_1: 0.1023 - recall_1: 0.7083 - val_loss: 0.3605 - val_precision_1: 0.1001 - val_recall_1: 0.6261\n"
     ]
    }
   ],
   "source": [
    "class_weight_model.compile(optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = [tf.keras.metrics.Precision(),\n",
    "                              tf.keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "history_cw = class_weight_model.fit(X_train, y_train, \n",
    "                         epochs = 32, \n",
    "                         batch_size= 128, \n",
    "                         validation_split= 0.2,\n",
    "                         class_weight= class_weights)\n",
    "                        # class_weight= {0: 0.5, 1: 32.948929159802304})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 0s 546us/step\n"
     ]
    }
   ],
   "source": [
    "preds = base_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     19712\n",
      "           1       0.72      0.29      0.42       288\n",
      "\n",
      "    accuracy                           0.99     20000\n",
      "   macro avg       0.86      0.65      0.70     20000\n",
      "weighted avg       0.99      0.99      0.99     20000\n",
      "\n",
      "[[19680    32]\n",
      " [  204    84]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "y_pred = np.where(preds >= threshold , 1 , 0)\n",
    "\n",
    "print(classification_report(y_test,  y_pred))\n",
    "print(confusion_matrix(y_test,  y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     19712\n",
      "           1       0.37      0.47      0.41       288\n",
      "\n",
      "    accuracy                           0.98     20000\n",
      "   macro avg       0.68      0.73      0.70     20000\n",
      "weighted avg       0.98      0.98      0.98     20000\n",
      "\n",
      "[[19478   234]\n",
      " [  152   136]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.1\n",
    "y_pred = np.where(preds >= threshold , 1 , 0)\n",
    "\n",
    "print(classification_report(y_test,  y_pred))\n",
    "print(confusion_matrix(y_test,  y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97     19712\n",
      "           1       0.13      0.64      0.22       288\n",
      "\n",
      "    accuracy                           0.93     20000\n",
      "   macro avg       0.56      0.79      0.59     20000\n",
      "weighted avg       0.98      0.93      0.95     20000\n",
      "\n",
      "[[18501  1211]\n",
      " [  104   184]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.02\n",
    "y_pred = np.where(preds >= threshold , 1 , 0)\n",
    "\n",
    "print(classification_report(y_test,  y_pred))\n",
    "print(confusion_matrix(y_test,  y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done for now"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.0010536, 1.03004  ], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [[0.1] , [0.8]]\n",
    "y_true = [[0] , [0]]\n",
    "\n",
    "tf.keras.losses.BinaryFocalCrossentropy(reduction='none')(y_true, y_pred) # Focal loss focusing wrong guesses. And punished so much. Like mse lol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reduce_mean([0.0010536, 1.03004])                    # np.array([0.0010536 , 1.03004]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.10536041, 1.6094375 ], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [[0.1] , [0.8]]\n",
    "y_true = [[0] , [0]]\n",
    "\n",
    "tf.keras.losses.BinaryCrossentropy(reduction='none')(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.10536041, 1.6094375 ], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [[0.1] , [0.8]]\n",
    "y_true = [[0] , [0]]\n",
    "\n",
    "tf.keras.losses.BinaryFocalCrossentropy(alpha=1 , gamma = 0 , reduction='none')(y_true, y_pred)  # alpha and gamma parameters is so important. if alpha = 1 , gamma = 0 equal to BinaryCrossentropy.\n",
    "                                                                                                                                             # if alpha = 0.25 , gamma = 2.0 equal to BinaryFocalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFocalCE(tf.keras.losses.Loss): # LossFunctionWrapper\n",
    "    def __init__(self, alpha = 0.25, gamma = 2.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def call(self , y_true , y_pred):\n",
    "        # y_true = tf.convert_to_tensor(y_true , dtype = tf.float32)\n",
    "        y_true = tf.cast(y_true , dtype = tf.float32)\n",
    "        # y_pred = tf.convert_to_tensor(y_pred , dtype = tf.float32)\n",
    "        y_pred = tf.cast(y_pred , dtype = tf.float32)\n",
    "        \n",
    "        # Objective maximize to likelihood.\n",
    "        # If we work with negative likelihood, we should minimize the -likelihood.\n",
    "        # We want to change multiply to add. We will use log-likelihood\n",
    "        # -log-likelihood   -->  negative log-likelihood\n",
    "        \n",
    "        y_pred = tf.clip_by_value(y_pred, \n",
    "                                  tf.keras.backend.epsilon(), \n",
    "                                  1-tf.keras.backend.epsilon())\n",
    "        \n",
    "        cce = -y_true * tf.math.log(y_pred) # Crossentropy\n",
    "        \n",
    "        # gamma is focusing parameter.\n",
    "        modulating_factor = tf.pow(1-y_pred , self.gamma)  \n",
    "        \n",
    "        # alpha weighting parameter \n",
    "        weighting_factor = tf.multiply(self.alpha, modulating_factor)\n",
    "        \n",
    "        # Focal Loss\n",
    "        focal_loss = tf.multiply(weighting_factor, cce)\n",
    "        \n",
    "        return focal_loss # return tf.recuce_mean(tf.reduce.sum(focal_loss, axis= -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.e-07, 9.e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.clip_by_value([[0 , 0.9]], tf.keras.backend.epsilon(), 1-tf.keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [[0.1, 0.9] , [0.8, 0.2]]\n",
    "y_true = [[0,1] , [0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.      ,  0.034769],\n",
       "       [ 0.      , 33.99133 ]], dtype=float32)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfce = CategoricalFocalCE(alpha= [0.5 , 33.0] , gamma = 2 , reduction = 'none') # reduction = 'none' is a parameter which works with **kwargs and it shows every punishment \n",
    "                                                                                # If gamma reduce, You increase the punishment which you know in data.\n",
    "cfce(y_true , y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.12888674>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.reduce_sum([[0.       , 0.0002634],\n",
    "       [0.       , 0.2575101]], axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 64)                704       \n",
      "                                                                 \n",
      " tf.nn.relu_12 (TFOpLambda)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " tf.nn.relu_13 (TFOpLambda)  (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 2)                 66        \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,850\n",
      "Trainable params: 2,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = tf.keras.Input(shape=(X.shape[1],))\n",
    "\n",
    "x = tf.keras.layers.Dense(64)(input_layer)\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(32)(x)\n",
    "x = tf.nn.relu(x)\n",
    "\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(2)(x)\n",
    "x = tf.keras.layers.Activation(\"softmax\")(x)\n",
    "\n",
    "\n",
    "categorical_focal_model = tf.keras.models.Model(input_layer, x)\n",
    "categorical_focal_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_focal_model.compile(optimizer = 'adam',\n",
    "                   loss = CategoricalFocalCE(alpha = [0.5, 33.0],\n",
    "                                             gamma = 2.0),\n",
    "                   metrics = [tf.keras.metrics.Precision(class_id = 1),\n",
    "                              tf.keras.metrics.Recall(class_id = 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0691 - precision_7: 0.0425 - recall_7: 0.6598 - val_loss: 0.0622 - val_precision_7: 0.0671 - val_recall_7: 0.6126\n",
      "Epoch 2/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0628 - precision_7: 0.0654 - recall_7: 0.6534 - val_loss: 0.0617 - val_precision_7: 0.0774 - val_recall_7: 0.6036\n",
      "Epoch 3/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0611 - precision_7: 0.0757 - recall_7: 0.6523 - val_loss: 0.0611 - val_precision_7: 0.0669 - val_recall_7: 0.6171\n",
      "Epoch 4/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0603 - precision_7: 0.0770 - recall_7: 0.6502 - val_loss: 0.0617 - val_precision_7: 0.0902 - val_recall_7: 0.5811\n",
      "Epoch 5/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0594 - precision_7: 0.0792 - recall_7: 0.6663 - val_loss: 0.0602 - val_precision_7: 0.0790 - val_recall_7: 0.6622\n",
      "Epoch 6/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0581 - precision_7: 0.0856 - recall_7: 0.6706 - val_loss: 0.0601 - val_precision_7: 0.0823 - val_recall_7: 0.6351\n",
      "Epoch 7/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0574 - precision_7: 0.0865 - recall_7: 0.6674 - val_loss: 0.0599 - val_precision_7: 0.0769 - val_recall_7: 0.6757\n",
      "Epoch 8/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0564 - precision_7: 0.0875 - recall_7: 0.6835 - val_loss: 0.0602 - val_precision_7: 0.1069 - val_recall_7: 0.5946\n",
      "Epoch 9/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0561 - precision_7: 0.0926 - recall_7: 0.6792 - val_loss: 0.0601 - val_precision_7: 0.1168 - val_recall_7: 0.5856\n",
      "Epoch 10/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0560 - precision_7: 0.0963 - recall_7: 0.6706 - val_loss: 0.0596 - val_precision_7: 0.1006 - val_recall_7: 0.6171\n",
      "Epoch 11/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0559 - precision_7: 0.0934 - recall_7: 0.6738 - val_loss: 0.0604 - val_precision_7: 0.1080 - val_recall_7: 0.5991\n",
      "Epoch 12/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0549 - precision_7: 0.1004 - recall_7: 0.6749 - val_loss: 0.0596 - val_precision_7: 0.0818 - val_recall_7: 0.6396\n",
      "Epoch 13/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0547 - precision_7: 0.0981 - recall_7: 0.6846 - val_loss: 0.0596 - val_precision_7: 0.0872 - val_recall_7: 0.6306\n",
      "Epoch 14/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0544 - precision_7: 0.1025 - recall_7: 0.6771 - val_loss: 0.0611 - val_precision_7: 0.1120 - val_recall_7: 0.5991\n",
      "Epoch 15/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0534 - precision_7: 0.1063 - recall_7: 0.6997 - val_loss: 0.0604 - val_precision_7: 0.0733 - val_recall_7: 0.6441\n",
      "Epoch 16/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0533 - precision_7: 0.0975 - recall_7: 0.6986 - val_loss: 0.0632 - val_precision_7: 0.0889 - val_recall_7: 0.5991\n",
      "Epoch 17/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0534 - precision_7: 0.1013 - recall_7: 0.7029 - val_loss: 0.0623 - val_precision_7: 0.0918 - val_recall_7: 0.6081\n",
      "Epoch 18/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0525 - precision_7: 0.1021 - recall_7: 0.6954 - val_loss: 0.0678 - val_precision_7: 0.1012 - val_recall_7: 0.5991\n",
      "Epoch 19/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0524 - precision_7: 0.0976 - recall_7: 0.7008 - val_loss: 0.0668 - val_precision_7: 0.1241 - val_recall_7: 0.5721\n",
      "Epoch 20/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0526 - precision_7: 0.1001 - recall_7: 0.7008 - val_loss: 0.0665 - val_precision_7: 0.1078 - val_recall_7: 0.5856\n",
      "Epoch 21/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0520 - precision_7: 0.1025 - recall_7: 0.6975 - val_loss: 0.0645 - val_precision_7: 0.1034 - val_recall_7: 0.6081\n",
      "Epoch 22/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0514 - precision_7: 0.1031 - recall_7: 0.6943 - val_loss: 0.0656 - val_precision_7: 0.0831 - val_recall_7: 0.6396\n",
      "Epoch 23/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0512 - precision_7: 0.1014 - recall_7: 0.7094 - val_loss: 0.0655 - val_precision_7: 0.0841 - val_recall_7: 0.6261\n",
      "Epoch 24/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0502 - precision_7: 0.1008 - recall_7: 0.7137 - val_loss: 0.0670 - val_precision_7: 0.0869 - val_recall_7: 0.6171\n",
      "Epoch 25/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0495 - precision_7: 0.1014 - recall_7: 0.7234 - val_loss: 0.0654 - val_precision_7: 0.0763 - val_recall_7: 0.6441\n",
      "Epoch 26/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0494 - precision_7: 0.0977 - recall_7: 0.7094 - val_loss: 0.0723 - val_precision_7: 0.1016 - val_recall_7: 0.5901\n",
      "Epoch 27/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0497 - precision_7: 0.0992 - recall_7: 0.7212 - val_loss: 0.0698 - val_precision_7: 0.0899 - val_recall_7: 0.6126\n",
      "Epoch 28/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0494 - precision_7: 0.1004 - recall_7: 0.7147 - val_loss: 0.0693 - val_precision_7: 0.0810 - val_recall_7: 0.6081\n",
      "Epoch 29/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0492 - precision_7: 0.0964 - recall_7: 0.7266 - val_loss: 0.0784 - val_precision_7: 0.1113 - val_recall_7: 0.5856\n",
      "Epoch 30/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0480 - precision_7: 0.0993 - recall_7: 0.7287 - val_loss: 0.0763 - val_precision_7: 0.0985 - val_recall_7: 0.5946\n",
      "Epoch 31/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0481 - precision_7: 0.1025 - recall_7: 0.7277 - val_loss: 0.0808 - val_precision_7: 0.1057 - val_recall_7: 0.5766\n",
      "Epoch 32/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0472 - precision_7: 0.1042 - recall_7: 0.7460 - val_loss: 0.0804 - val_precision_7: 0.0881 - val_recall_7: 0.6081\n",
      "Epoch 33/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0479 - precision_7: 0.0949 - recall_7: 0.7320 - val_loss: 0.0752 - val_precision_7: 0.0762 - val_recall_7: 0.6261\n",
      "Epoch 34/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0469 - precision_7: 0.1017 - recall_7: 0.7320 - val_loss: 0.0753 - val_precision_7: 0.0875 - val_recall_7: 0.6081\n",
      "Epoch 35/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0469 - precision_7: 0.0990 - recall_7: 0.7341 - val_loss: 0.0858 - val_precision_7: 0.1005 - val_recall_7: 0.5946\n",
      "Epoch 36/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0460 - precision_7: 0.1004 - recall_7: 0.7460 - val_loss: 0.0901 - val_precision_7: 0.1044 - val_recall_7: 0.5856\n",
      "Epoch 37/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0456 - precision_7: 0.1018 - recall_7: 0.7438 - val_loss: 0.0831 - val_precision_7: 0.0658 - val_recall_7: 0.6396\n",
      "Epoch 38/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0457 - precision_7: 0.0941 - recall_7: 0.7643 - val_loss: 0.0926 - val_precision_7: 0.0858 - val_recall_7: 0.6081\n",
      "Epoch 39/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0457 - precision_7: 0.0965 - recall_7: 0.7427 - val_loss: 0.0951 - val_precision_7: 0.0890 - val_recall_7: 0.5991\n",
      "Epoch 40/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0447 - precision_7: 0.0979 - recall_7: 0.7632 - val_loss: 0.0805 - val_precision_7: 0.0840 - val_recall_7: 0.6216\n",
      "Epoch 41/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0448 - precision_7: 0.1003 - recall_7: 0.7707 - val_loss: 0.0935 - val_precision_7: 0.0787 - val_recall_7: 0.6171\n",
      "Epoch 42/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0441 - precision_7: 0.0932 - recall_7: 0.7750 - val_loss: 0.1030 - val_precision_7: 0.1010 - val_recall_7: 0.5946\n",
      "Epoch 43/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0442 - precision_7: 0.0938 - recall_7: 0.7772 - val_loss: 0.0993 - val_precision_7: 0.0875 - val_recall_7: 0.5946\n",
      "Epoch 44/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0442 - precision_7: 0.0990 - recall_7: 0.7718 - val_loss: 0.0922 - val_precision_7: 0.0716 - val_recall_7: 0.6171\n",
      "Epoch 45/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0437 - precision_7: 0.0952 - recall_7: 0.7783 - val_loss: 0.1003 - val_precision_7: 0.0896 - val_recall_7: 0.5946\n",
      "Epoch 46/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0426 - precision_7: 0.0952 - recall_7: 0.7772 - val_loss: 0.1081 - val_precision_7: 0.0771 - val_recall_7: 0.5856\n",
      "Epoch 47/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0428 - precision_7: 0.0942 - recall_7: 0.7750 - val_loss: 0.1043 - val_precision_7: 0.0794 - val_recall_7: 0.5946\n",
      "Epoch 48/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0428 - precision_7: 0.0959 - recall_7: 0.7858 - val_loss: 0.0988 - val_precision_7: 0.0778 - val_recall_7: 0.6036\n",
      "Epoch 49/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0425 - precision_7: 0.0947 - recall_7: 0.7847 - val_loss: 0.1147 - val_precision_7: 0.0744 - val_recall_7: 0.5811\n",
      "Epoch 50/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0417 - precision_7: 0.0934 - recall_7: 0.7890 - val_loss: 0.1085 - val_precision_7: 0.0750 - val_recall_7: 0.5946\n",
      "Epoch 51/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0418 - precision_7: 0.0961 - recall_7: 0.7858 - val_loss: 0.1173 - val_precision_7: 0.0825 - val_recall_7: 0.5901\n",
      "Epoch 52/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0413 - precision_7: 0.0976 - recall_7: 0.8019 - val_loss: 0.1125 - val_precision_7: 0.0705 - val_recall_7: 0.5811\n",
      "Epoch 53/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0410 - precision_7: 0.0960 - recall_7: 0.7879 - val_loss: 0.1244 - val_precision_7: 0.0817 - val_recall_7: 0.5946\n",
      "Epoch 54/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0412 - precision_7: 0.0934 - recall_7: 0.7922 - val_loss: 0.1183 - val_precision_7: 0.0646 - val_recall_7: 0.5946\n",
      "Epoch 55/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0411 - precision_7: 0.0907 - recall_7: 0.7998 - val_loss: 0.1289 - val_precision_7: 0.0795 - val_recall_7: 0.5856\n",
      "Epoch 56/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0418 - precision_7: 0.0932 - recall_7: 0.8062 - val_loss: 0.1221 - val_precision_7: 0.0712 - val_recall_7: 0.5856\n",
      "Epoch 57/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0399 - precision_7: 0.0968 - recall_7: 0.8159 - val_loss: 0.1409 - val_precision_7: 0.0818 - val_recall_7: 0.5721\n",
      "Epoch 58/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0413 - precision_7: 0.0924 - recall_7: 0.8019 - val_loss: 0.1281 - val_precision_7: 0.0760 - val_recall_7: 0.5856\n",
      "Epoch 59/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0399 - precision_7: 0.0979 - recall_7: 0.8116 - val_loss: 0.1461 - val_precision_7: 0.0945 - val_recall_7: 0.5721\n",
      "Epoch 60/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0402 - precision_7: 0.0928 - recall_7: 0.8009 - val_loss: 0.1456 - val_precision_7: 0.0818 - val_recall_7: 0.5856\n",
      "Epoch 61/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0393 - precision_7: 0.0983 - recall_7: 0.8181 - val_loss: 0.1428 - val_precision_7: 0.0773 - val_recall_7: 0.5901\n",
      "Epoch 62/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0391 - precision_7: 0.0942 - recall_7: 0.8095 - val_loss: 0.1544 - val_precision_7: 0.0749 - val_recall_7: 0.5631\n",
      "Epoch 63/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0392 - precision_7: 0.0969 - recall_7: 0.8116 - val_loss: 0.1304 - val_precision_7: 0.0713 - val_recall_7: 0.6036\n",
      "Epoch 64/64\n",
      "500/500 [==============================] - 1s 2ms/step - loss: 0.0394 - precision_7: 0.0916 - recall_7: 0.8149 - val_loss: 0.1447 - val_precision_7: 0.0750 - val_recall_7: 0.5676\n"
     ]
    }
   ],
   "source": [
    "y_train_ohe = tf.keras.utils.to_categorical(y_train,2)\n",
    "\n",
    "history = categorical_focal_model.fit(X_train, y_train_ohe, epochs = 64, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 777us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9339436 , 0.06605638],\n",
       "       [0.80724615, 0.19275391],\n",
       "       [0.75660473, 0.24339531],\n",
       "       ...,\n",
       "       [0.79793024, 0.20206973],\n",
       "       [0.52278113, 0.47721884],\n",
       "       [0.90780574, 0.09219427]], dtype=float32)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = categorical_focal_model.predict(X_test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(preds, axis= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test_ohe = tf.keras.utils.to_categorical(y_test , 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9929    0.8952    0.9415     19712\n",
      "           1     0.0727    0.5625    0.1288       288\n",
      "\n",
      "    accuracy                         0.8904     20000\n",
      "   macro avg     0.5328    0.7289    0.5352     20000\n",
      "weighted avg     0.9797    0.8904    0.9298     20000\n",
      "\n",
      "[[17647  2065]\n",
      " [  126   162]]\n"
     ]
    }
   ],
   "source": [
    "preds_classes = np.argmax(preds, axis= -1)\n",
    "print(classification_report(y_test, preds_classes, digits= 4))\n",
    "print(confusion_matrix(y_test, preds_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done for now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
