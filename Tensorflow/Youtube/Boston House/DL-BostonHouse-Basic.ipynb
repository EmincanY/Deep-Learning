{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train , y_train) , (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (404, 13)\n",
      "Test Data : (102, 13)\n",
      "Train Sample : [  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]\n",
      "Test Sample : [ 18.0846   0.      18.1      0.       0.679    6.434  100.       1.8347\n",
      "  24.     666.      20.2     27.25    29.05  ]\n"
     ]
    }
   ],
   "source": [
    "print(f'Training Data : {X_train.shape}')\n",
    "print(f'Test Data : {X_test.shape}')\n",
    "print(f'Train Sample : {X_train[0]}')\n",
    "print(f'Test Sample : {X_test[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1.Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "# Note that the quantities used for normalizing the test data are computed using the\n",
    "# training data. You should never use in your workflow any quantity computed on the\n",
    "# test data, even for something as simple as data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler # MinMaxScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "def build_simple_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation = 'relu' , input_shape = (X_train.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation = 'relu'))\n",
    "    model.add(layers.Dense(32, activation = 'relu'))\n",
    "    \n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold #0\n",
      "Processing fold #1\n",
      "Processing fold #2\n",
      "Processing fold #3\n",
      "Processing fold #4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 5\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "model = build_simple_model()\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss=losses.MeanSquaredError(),\n",
    "              metrics=['mae'])\n",
    "\n",
    "# Perform K-Fold cross validation\n",
    "kf = KFold(n_splits=k)\n",
    "for i, (train_index, val_index) in enumerate(kf.split(X_train)):\n",
    "    print(f'Processing fold #{i}')\n",
    "\n",
    "    # Split data into training and validation sets\n",
    "    partial_train_data, val_data = X_train[train_index], X_train[val_index]\n",
    "    partial_train_targets, val_targets = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    _, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)\n",
    "\n",
    "    # Reset the model's weights\n",
    "    model.set_weights(build_simple_model().get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_scores : [1.9217571020126343, 2.3364145755767822, 2.1722605228424072, 2.582444190979004, 2.1032001972198486]\n",
      "mean all scores : 2.2232153177261353\n"
     ]
    }
   ],
   "source": [
    "print(f'all_scores : {all_scores}')\n",
    "print(f'mean all scores : {np.mean(all_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "51/51 [==============================] - 0s 3ms/step - loss: 330.8762 - mae: 15.2810\n",
      "Epoch 2/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 48.9057 - mae: 4.9657\n",
      "Epoch 3/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 22.6340 - mae: 3.2926\n",
      "Epoch 4/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 17.9044 - mae: 3.0373\n",
      "Epoch 5/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 15.2735 - mae: 2.8034\n",
      "Epoch 6/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 13.2503 - mae: 2.5478\n",
      "Epoch 7/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 12.4915 - mae: 2.4868\n",
      "Epoch 8/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 11.1325 - mae: 2.4153\n",
      "Epoch 9/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 10.7269 - mae: 2.3339\n",
      "Epoch 10/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 9.9893 - mae: 2.3062\n",
      "Epoch 11/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 9.8071 - mae: 2.2695\n",
      "Epoch 12/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 9.4087 - mae: 2.2187\n",
      "Epoch 13/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 9.0195 - mae: 2.1435\n",
      "Epoch 14/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 8.8310 - mae: 2.1311\n",
      "Epoch 15/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 8.8170 - mae: 2.1453\n",
      "Epoch 16/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 8.6458 - mae: 2.1275\n",
      "Epoch 17/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 7.9253 - mae: 2.0305\n",
      "Epoch 18/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 8.2781 - mae: 2.0867\n",
      "Epoch 19/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 7.6012 - mae: 2.0437\n",
      "Epoch 20/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.9925 - mae: 1.8781\n",
      "Epoch 21/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 7.3448 - mae: 1.9300\n",
      "Epoch 22/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 7.9576 - mae: 2.0305\n",
      "Epoch 23/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 7.0038 - mae: 1.9049\n",
      "Epoch 24/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.6525 - mae: 1.8457\n",
      "Epoch 25/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.6264 - mae: 1.8662\n",
      "Epoch 26/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.8675 - mae: 1.9281\n",
      "Epoch 27/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.4449 - mae: 1.8112\n",
      "Epoch 28/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.0306 - mae: 1.7551\n",
      "Epoch 29/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.0425 - mae: 1.7948\n",
      "Epoch 30/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 6.0855 - mae: 1.7855\n",
      "Epoch 31/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 6.1321 - mae: 1.7725\n",
      "Epoch 32/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.7737 - mae: 1.7254\n",
      "Epoch 33/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.5672 - mae: 1.7019\n",
      "Epoch 34/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 5.2052 - mae: 1.6049\n",
      "Epoch 35/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.0120 - mae: 1.6146\n",
      "Epoch 36/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.5320 - mae: 1.6964\n",
      "Epoch 37/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.5352 - mae: 1.7034\n",
      "Epoch 38/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.9080 - mae: 1.6107\n",
      "Epoch 39/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 5.0189 - mae: 1.6570\n",
      "Epoch 40/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.8557 - mae: 1.6145\n",
      "Epoch 41/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.8854 - mae: 1.6065\n",
      "Epoch 42/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.4664 - mae: 1.5739\n",
      "Epoch 43/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.7839 - mae: 1.6334\n",
      "Epoch 44/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.0541 - mae: 1.4872\n",
      "Epoch 45/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.0966 - mae: 1.5018\n",
      "Epoch 46/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.0587 - mae: 1.4903\n",
      "Epoch 47/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.0697 - mae: 1.5237\n",
      "Epoch 48/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.4077 - mae: 1.5832\n",
      "Epoch 49/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.9546 - mae: 1.4967\n",
      "Epoch 50/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 3.7862 - mae: 1.4320\n",
      "Epoch 51/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.1256 - mae: 1.4926\n",
      "Epoch 52/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.2735 - mae: 1.5888\n",
      "Epoch 53/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.8473 - mae: 1.4418\n",
      "Epoch 54/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.6321 - mae: 1.4392\n",
      "Epoch 55/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.3678 - mae: 1.3853\n",
      "Epoch 56/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.7921 - mae: 1.4591\n",
      "Epoch 57/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.4844 - mae: 1.3880\n",
      "Epoch 58/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 4.4549 - mae: 1.5844\n",
      "Epoch 59/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.6106 - mae: 1.4261\n",
      "Epoch 60/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.5686 - mae: 1.4401\n",
      "Epoch 61/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.3405 - mae: 1.3671\n",
      "Epoch 62/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.2640 - mae: 1.3734\n",
      "Epoch 63/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9938 - mae: 1.3171\n",
      "Epoch 64/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9911 - mae: 1.2921\n",
      "Epoch 65/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9222 - mae: 1.2888\n",
      "Epoch 66/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9464 - mae: 1.3016\n",
      "Epoch 67/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 2.9941 - mae: 1.2903\n",
      "Epoch 68/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.2108 - mae: 1.3735\n",
      "Epoch 69/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.8307 - mae: 1.2504\n",
      "Epoch 70/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.8789 - mae: 1.2678\n",
      "Epoch 71/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.6274 - mae: 1.2053\n",
      "Epoch 72/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.6499 - mae: 1.2250\n",
      "Epoch 73/80\n",
      "51/51 [==============================] - 0s 1ms/step - loss: 3.5662 - mae: 1.4347\n",
      "Epoch 74/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.7876 - mae: 1.2436\n",
      "Epoch 75/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 3.2806 - mae: 1.3905\n",
      "Epoch 76/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9480 - mae: 1.2831\n",
      "Epoch 77/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.7428 - mae: 1.2492\n",
      "Epoch 78/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.8280 - mae: 1.2672\n",
      "Epoch 79/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.9982 - mae: 1.3212\n",
      "Epoch 80/80\n",
      "51/51 [==============================] - 0s 2ms/step - loss: 2.3354 - mae: 1.1206\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 11.9957 - mae: 2.3817\n"
     ]
    }
   ],
   "source": [
    "model = build_simple_model()\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "              loss=losses.MeanSquaredError(),\n",
    "              metrics=['mae'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=80, batch_size=8, verbose=1)\n",
    "\n",
    "test_mse_score, test_mae_score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
